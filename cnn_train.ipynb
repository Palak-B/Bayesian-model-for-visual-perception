{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1e85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1bb887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed\n",
    "torch.manual_seed(11)\n",
    "\n",
    "#load MNIST training and test datasets\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data',\n",
    "                              train=True,\n",
    "                              download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "test_dataset = datasets.MNIST(root='data',\n",
    "                              train=False,\n",
    "                              download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809083b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training dataset into training and validation sets\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, \n",
    "                                                           [50000, 10000],\n",
    "                                                           generator=torch.Generator().manual_seed(11))\n",
    "\n",
    "#create filters for datasets so only 5s and 8s are included\n",
    "train_filter = [idx for idx, sample in enumerate(train_dataset) if sample[1] in [5,8]]\n",
    "val_filter = [idx for idx, sample in enumerate(val_dataset) if sample[1] in [5,8]]\n",
    "\n",
    "\n",
    "#create dataloaders using filtered training and test datasets\n",
    "train_dataloader = torch.utils.data.DataLoader(torch.utils.data.Subset(train_dataset, train_filter),\n",
    "                                              batch_size = 64)\n",
    "val_dataloader = torch.utils.data.DataLoader(torch.utils.data.Subset(val_dataset, val_filter),\n",
    "                                              batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7bbf691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training labels:  tensor([8, 8, 5, 5, 8, 8, 5, 5, 8, 5, 5, 8, 5, 5, 5, 5, 5, 5, 8, 5, 5, 8, 5, 8,\n",
      "        8, 5, 5, 8, 5, 8, 8, 8, 5, 5, 8, 8, 5, 5, 5, 5, 8, 8, 8, 8, 5, 8, 5, 8,\n",
      "        8, 8, 8, 8, 5, 5, 5, 5, 8, 8, 8, 8, 5, 8, 5, 5])\n",
      "val labels:  tensor([5, 5, 5, 5, 5, 8, 8, 8, 5, 5, 8, 8, 5, 5, 8, 5, 8, 8, 5, 8, 5, 8, 8, 5,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 5, 8, 5, 5, 5, 8, 8, 8, 8, 8, 5, 8, 5,\n",
      "        8, 8, 8, 5, 5, 8, 5, 5, 8, 5, 5, 8, 8, 5, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "#check to make sure dataloaders are properly filtered\n",
    "\n",
    "train_labels = next(iter(train_dataloader))[1]\n",
    "print('training labels: ', train_labels)\n",
    "\n",
    "val_labels = next(iter(val_dataloader))[1]\n",
    "print('val labels: ', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc2049e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define network for image processing\n",
    "#outputs are in the form of (log class probabilities, hidden features)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # 10 channels in first convolution layer\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) # 20 channels in second conv. layer\n",
    "        self.fc1 = nn.Linear(320, 10) # 10 hidden units in first fully-connected layer\n",
    "        self.fc2 = nn.Linear(10, 2) # 2 output units\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # first convolutional layer\n",
    "        h_conv1 = self.conv1(x)\n",
    "        h_conv1 = F.relu(h_conv1)\n",
    "        h_conv1_pool = F.max_pool2d(h_conv1, 2)\n",
    "\n",
    "        # second convolutional layer\n",
    "        h_conv2 = self.conv2(h_conv1_pool)\n",
    "        h_conv2 = F.relu(h_conv2)\n",
    "        h_conv2_pool = F.max_pool2d(h_conv2, 2)\n",
    "\n",
    "        # fully-connected layer\n",
    "        h_fc1 = h_conv2_pool.view(-1, 320)\n",
    "        h_fc1 = self.fc1(h_fc1)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "        \n",
    "        # classifier output\n",
    "        output = self.fc2(h_fc1)\n",
    "        output = F.log_softmax(output,dim=1)\n",
    "        return output, h_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60ea82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training loop\n",
    "\n",
    "def train_one_epoch():\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        data = batch[0]\n",
    "        target = batch[1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[0]\n",
    "        \n",
    "        #relabeling target values so 5-->0 and 8-->1\n",
    "        new_target = torch.tensor([0 if label==5 else 1 for label in target])\n",
    "        \n",
    "        loss = criterion(output, new_target)\n",
    "        #print('loss: ', loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i%10 == 9:\n",
    "            last_loss = running_loss/10\n",
    "            print('batch {} loss: {}'.format(i+1, last_loss))\n",
    "            running_loss = 0.\n",
    "        \n",
    "            \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f82af24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "batch 10 loss: 0.6660315454006195\n",
      "batch 20 loss: 0.55852070748806\n",
      "batch 30 loss: 0.3826448380947113\n",
      "batch 40 loss: 0.20611062049865722\n",
      "batch 50 loss: 0.14634240940213203\n",
      "batch 60 loss: 0.06813690923154354\n",
      "batch 70 loss: 0.09970612227916717\n",
      "batch 80 loss: 0.06790848411619663\n",
      "batch 90 loss: 0.04834919441491366\n",
      "batch 100 loss: 0.050461013056337836\n",
      "batch 110 loss: 0.037664445023983716\n",
      "batch 120 loss: 0.04826828418299556\n",
      "batch 130 loss: 0.04373614629730582\n",
      "batch 140 loss: 0.04619812034070492\n",
      "LOSS train 0.04619812034070492 valid 0.04880813509225845\n",
      "EPOCH 2\n",
      "batch 10 loss: 0.03526162952184677\n",
      "batch 20 loss: 0.048914133291691544\n",
      "batch 30 loss: 0.03928437726572156\n",
      "batch 40 loss: 0.041596430353820324\n",
      "batch 50 loss: 0.04085503877140582\n",
      "batch 60 loss: 0.014235061220824718\n",
      "batch 70 loss: 0.045776332076638934\n",
      "batch 80 loss: 0.03707020878791809\n",
      "batch 90 loss: 0.020973848761059344\n",
      "batch 100 loss: 0.01804286534897983\n",
      "batch 110 loss: 0.019134345883503556\n",
      "batch 120 loss: 0.015225190436467528\n",
      "batch 130 loss: 0.014293980738148093\n",
      "batch 140 loss: 0.024358427873812614\n",
      "LOSS train 0.024358427873812614 valid 0.02103731408715248\n",
      "EPOCH 3\n",
      "batch 10 loss: 0.018556972360238434\n",
      "batch 20 loss: 0.037266148207709196\n",
      "batch 30 loss: 0.029232552321627737\n",
      "batch 40 loss: 0.026591691421344876\n",
      "batch 50 loss: 0.0327353774337098\n",
      "batch 60 loss: 0.008610804029740394\n",
      "batch 70 loss: 0.038017444871366025\n",
      "batch 80 loss: 0.035868634656071666\n",
      "batch 90 loss: 0.015547100035473705\n",
      "batch 100 loss: 0.012193690508138389\n",
      "batch 110 loss: 0.017881791689433157\n",
      "batch 120 loss: 0.011071543325670063\n",
      "batch 130 loss: 0.006769121321849525\n",
      "batch 140 loss: 0.015131164342164993\n",
      "LOSS train 0.015131164342164993 valid 0.018243713304400444\n",
      "EPOCH 4\n",
      "batch 10 loss: 0.012717828527092934\n",
      "batch 20 loss: 0.024706260394304992\n",
      "batch 30 loss: 0.019627033406868576\n",
      "batch 40 loss: 0.02027962721185759\n",
      "batch 50 loss: 0.0229314204712864\n",
      "batch 60 loss: 0.007068622711813077\n",
      "batch 70 loss: 0.024377624917542563\n",
      "batch 80 loss: 0.021140399447176605\n",
      "batch 90 loss: 0.008155757409986109\n",
      "batch 100 loss: 0.010923767159692944\n",
      "batch 110 loss: 0.01721430215984583\n",
      "batch 120 loss: 0.010550990735646337\n",
      "batch 130 loss: 0.005882954399567097\n",
      "batch 140 loss: 0.012637364820693619\n",
      "LOSS train 0.012637364820693619 valid 0.021899674087762833\n",
      "EPOCH 5\n",
      "batch 10 loss: 0.007812245102832094\n",
      "batch 20 loss: 0.015690143883693962\n",
      "batch 30 loss: 0.016368853591848163\n",
      "batch 40 loss: 0.012411114812130109\n",
      "batch 50 loss: 0.0178264427697286\n",
      "batch 60 loss: 0.00395168163231574\n",
      "batch 70 loss: 0.01882170298777055\n",
      "batch 80 loss: 0.012904579192399979\n",
      "batch 90 loss: 0.007039012026507407\n",
      "batch 100 loss: 0.008171974058495835\n",
      "batch 110 loss: 0.012373072106856852\n",
      "batch 120 loss: 0.0069380126427859064\n",
      "batch 130 loss: 0.003663008933654055\n",
      "batch 140 loss: 0.008616723271552473\n",
      "LOSS train 0.008616723271552473 valid 0.019138960167765617\n"
     ]
    }
   ],
   "source": [
    "#set training parameters\n",
    "model = CNN()\n",
    "criterion = torch.nn.NLLLoss()\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "num_epochs = 5\n",
    "\n",
    "best_val_loss = 10e6\n",
    "\n",
    "#train model\n",
    "for epoch in range(num_epochs):\n",
    "    print('EPOCH {}'.format(epoch+1))\n",
    "    \n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch()\n",
    "    \n",
    "    #check validation loss\n",
    "    model.train(False)\n",
    "    running_val_loss = 0.\n",
    "    for i, val_data in enumerate(val_dataloader):\n",
    "        val_inputs, val_target = val_data\n",
    "        val_outputs = model(val_inputs)[0]\n",
    "        \n",
    "        #relabeling target values so 5-->0 and 8-->1\n",
    "        new_val_target = torch.tensor([0 if label==5 else 1 for label in val_target])\n",
    "        \n",
    "        val_loss = criterion(val_outputs, new_val_target)\n",
    "        running_val_loss += val_loss\n",
    "    \n",
    "    avg_val_loss = running_val_loss / (i+1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_val_loss))\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b21f1fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
      "0      8        0.0  11.246423        0.0   7.995950   9.410959  19.745337   \n",
      "1      8        0.0  13.312637        0.0  10.743755  11.378502  18.715256   \n",
      "2      5        0.0   0.000000        0.0   0.000000   0.000000  28.648830   \n",
      "3      5        0.0   0.000000        0.0   0.000000   0.000000  26.991035   \n",
      "4      8        0.0   5.038842        0.0   5.323320   5.368689  22.266983   \n",
      "\n",
      "   feature_7  feature_8  feature_9  feature_10  \n",
      "0   9.010060   8.527996  10.110795   19.589340  \n",
      "1  11.810072  11.065614  12.528616   18.708977  \n",
      "2   0.000000   0.000000   0.000000   30.524609  \n",
      "3   0.000000   0.000000   0.000000   29.041384  \n",
      "4   4.806798   4.124915   4.903548   23.818264  \n"
     ]
    }
   ],
   "source": [
    "#extract hidden features from training dataset\n",
    "\n",
    "#initialize results array\n",
    "hidden_features_results = np.empty([len(train_dataloader)*64, 11])\n",
    "\n",
    "#iterate through training dataloader and collect hidden features\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    data = batch[0]\n",
    "    target = batch[1]\n",
    "    \n",
    "    log_probs, hidden_features = model(data)\n",
    "    \n",
    "    batch_results = np.empty([len(target), 11])\n",
    "    \n",
    "    batch_results[:,0] = target\n",
    "    batch_results[:,1:] = hidden_features.detach().numpy()\n",
    "    \n",
    "    hidden_features_results[i*64:(i*64)+len(target)] = batch_results\n",
    "    \n",
    "#store results in dataframe\n",
    "cols = ['label',\n",
    "        'feature_1',\n",
    "        'feature_2',\n",
    "        'feature_3',\n",
    "        'feature_4',\n",
    "        'feature_5',\n",
    "        'feature_6',\n",
    "        'feature_7',\n",
    "        'feature_8',\n",
    "        'feature_9',\n",
    "        'feature_10']\n",
    "\n",
    "hidden_features_df = pd.DataFrame(data = hidden_features_results, columns = cols)\n",
    "hidden_features_df['label'] = hidden_features_df['label'].astype(int)\n",
    "#hidden_features_df = hidden_features_df[np.logical_or(hidden_features_df['label']==5, hidden_features_df['label']==8)]\n",
    "print(hidden_features_df.head())\n",
    "\n",
    "#save dataframe as csv\n",
    "hidden_features_df.to_csv('hidden_features_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c829e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
