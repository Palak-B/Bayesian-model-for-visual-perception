{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1e85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1bb887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MNIST training and test datasets\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data',\n",
    "                              train=True,\n",
    "                              download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "test_dataset = datasets.MNIST(root='data',\n",
    "                              train=False,\n",
    "                              download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809083b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training dataset into training and validation sets\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, \n",
    "                                                           [50000, 10000],\n",
    "                                                           generator=torch.Generator().manual_seed(11))\n",
    "\n",
    "#create filters for datasets so only 5s and 8s are included\n",
    "train_filter = [idx for idx, sample in enumerate(train_dataset) if sample[1] in [5,8]]\n",
    "val_filter = [idx for idx, sample in enumerate(val_dataset) if sample[1] in [5,8]]\n",
    "\n",
    "\n",
    "#create dataloaders using filtered training and test datasets\n",
    "train_dataloader = torch.utils.data.DataLoader(torch.utils.data.Subset(train_dataset, train_filter),\n",
    "                                              batch_size = 64)\n",
    "val_dataloader = torch.utils.data.DataLoader(torch.utils.data.Subset(val_dataset, val_filter),\n",
    "                                              batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7bbf691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training labels:  tensor([8, 8, 5, 5, 8, 8, 5, 5, 8, 5, 5, 8, 5, 5, 5, 5, 5, 5, 8, 5, 5, 8, 5, 8,\n",
      "        8, 5, 5, 8, 5, 8, 8, 8, 5, 5, 8, 8, 5, 5, 5, 5, 8, 8, 8, 8, 5, 8, 5, 8,\n",
      "        8, 8, 8, 8, 5, 5, 5, 5, 8, 8, 8, 8, 5, 8, 5, 5])\n",
      "val labels:  tensor([5, 5, 5, 5, 5, 8, 8, 8, 5, 5, 8, 8, 5, 5, 8, 5, 8, 8, 5, 8, 5, 8, 8, 5,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 5, 8, 5, 5, 5, 8, 8, 8, 8, 8, 5, 8, 5,\n",
      "        8, 8, 8, 5, 5, 8, 5, 5, 8, 5, 5, 8, 8, 5, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "#check to make sure dataloaders are properly filtered\n",
    "\n",
    "train_labels = next(iter(train_dataloader))[1]\n",
    "print('training labels: ', train_labels)\n",
    "\n",
    "val_labels = next(iter(val_dataloader))[1]\n",
    "print('val labels: ', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc2049e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define network for image processing\n",
    "#outputs are in the form of (log class probabilities, hidden features)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # 10 channels in first convolution layer\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) # 20 channels in second conv. layer\n",
    "        self.fc1 = nn.Linear(320, 10) # 10 hidden units in first fully-connected layer\n",
    "        self.fc2 = nn.Linear(10, 2) # 2 output units\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # first convolutional layer\n",
    "        h_conv1 = self.conv1(x)\n",
    "        h_conv1 = F.relu(h_conv1)\n",
    "        h_conv1_pool = F.max_pool2d(h_conv1, 2)\n",
    "\n",
    "        # second convolutional layer\n",
    "        h_conv2 = self.conv2(h_conv1_pool)\n",
    "        h_conv2 = F.relu(h_conv2)\n",
    "        h_conv2_pool = F.max_pool2d(h_conv2, 2)\n",
    "\n",
    "        # fully-connected layer\n",
    "        h_fc1 = h_conv2_pool.view(-1, 320)\n",
    "        h_fc1 = self.fc1(h_fc1)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "        \n",
    "        # classifier output\n",
    "        output = self.fc2(h_fc1)\n",
    "        output = F.log_softmax(output,dim=1)\n",
    "        return output, h_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60ea82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training loop\n",
    "\n",
    "def train_one_epoch():\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        data = batch[0]\n",
    "        target = batch[1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[0]\n",
    "        \n",
    "        #relabeling target values so 5-->0 and 8-->1\n",
    "        new_target = torch.tensor([0 if label==5 else 1 for label in target])\n",
    "        \n",
    "        loss = criterion(output, new_target)\n",
    "        #print('loss: ', loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i%10 == 9:\n",
    "            last_loss = running_loss/10\n",
    "            print('batch {} loss: {}'.format(i+1, last_loss))\n",
    "            running_loss = 0.\n",
    "        \n",
    "            \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f82af24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "batch 10 loss: 0.6243726432323455\n",
      "batch 20 loss: 0.4372499108314514\n",
      "batch 30 loss: 0.23612861186265946\n",
      "batch 40 loss: 0.1541304975748062\n",
      "batch 50 loss: 0.1390708189457655\n",
      "batch 60 loss: 0.060294268652796745\n",
      "batch 70 loss: 0.10320460312068462\n",
      "batch 80 loss: 0.06572298742830754\n",
      "batch 90 loss: 0.04270720221102238\n",
      "batch 100 loss: 0.04171708486974239\n",
      "batch 110 loss: 0.03367207907140255\n",
      "batch 120 loss: 0.03739005010575056\n",
      "batch 130 loss: 0.042929185554385185\n",
      "batch 140 loss: 0.03326898450031877\n",
      "LOSS train 0.03326898450031877 valid 0.04624592140316963\n",
      "EPOCH 2\n",
      "batch 10 loss: 0.028517298866063356\n",
      "batch 20 loss: 0.0439563823863864\n",
      "batch 30 loss: 0.034433736279606816\n",
      "batch 40 loss: 0.03420436764135957\n",
      "batch 50 loss: 0.04106012156698853\n",
      "batch 60 loss: 0.011098131444305182\n",
      "batch 70 loss: 0.04802835620939731\n",
      "batch 80 loss: 0.04267579903826117\n",
      "batch 90 loss: 0.019288263004273175\n",
      "batch 100 loss: 0.011481682816520334\n",
      "batch 110 loss: 0.01902317611966282\n",
      "batch 120 loss: 0.014612987637519836\n",
      "batch 130 loss: 0.015038621868006885\n",
      "batch 140 loss: 0.02149727726355195\n",
      "LOSS train 0.02149727726355195 valid 0.02210494503378868\n",
      "EPOCH 3\n",
      "batch 10 loss: 0.011784515623003245\n",
      "batch 20 loss: 0.02780067455023527\n",
      "batch 30 loss: 0.024530400871299207\n",
      "batch 40 loss: 0.019988393073435872\n",
      "batch 50 loss: 0.02987419825512916\n",
      "batch 60 loss: 0.007230377628002316\n",
      "batch 70 loss: 0.03539350349456072\n",
      "batch 80 loss: 0.03151625916361809\n",
      "batch 90 loss: 0.012899360246956348\n",
      "batch 100 loss: 0.006888963002711535\n",
      "batch 110 loss: 0.017469437699764966\n",
      "batch 120 loss: 0.01192139128688723\n",
      "batch 130 loss: 0.009649655886460096\n",
      "batch 140 loss: 0.013300194556359201\n",
      "LOSS train 0.013300194556359201 valid 0.01865522377192974\n",
      "EPOCH 4\n",
      "batch 10 loss: 0.007125507073942572\n",
      "batch 20 loss: 0.01960264832014218\n",
      "batch 30 loss: 0.01800636468688026\n",
      "batch 40 loss: 0.013717514369636774\n",
      "batch 50 loss: 0.019526295055402442\n",
      "batch 60 loss: 0.006646107530104928\n",
      "batch 70 loss: 0.026984274329151958\n",
      "batch 80 loss: 0.019495897553861143\n",
      "batch 90 loss: 0.00942772023845464\n",
      "batch 100 loss: 0.006316772114951163\n",
      "batch 110 loss: 0.016495975432917476\n",
      "batch 120 loss: 0.008137362985871732\n",
      "batch 130 loss: 0.007110018143430352\n",
      "batch 140 loss: 0.008211599808419123\n",
      "LOSS train 0.008211599808419123 valid 0.01705125905573368\n",
      "EPOCH 5\n",
      "batch 10 loss: 0.004165352298878133\n",
      "batch 20 loss: 0.012151989818084985\n",
      "batch 30 loss: 0.013315236987546086\n",
      "batch 40 loss: 0.009253104633535259\n",
      "batch 50 loss: 0.0133358002116438\n",
      "batch 60 loss: 0.005400043779809493\n",
      "batch 70 loss: 0.022087095395545476\n",
      "batch 80 loss: 0.013944408018141985\n",
      "batch 90 loss: 0.008125515008578077\n",
      "batch 100 loss: 0.004914517732686363\n",
      "batch 110 loss: 0.01429268999490887\n",
      "batch 120 loss: 0.0060510940267704426\n",
      "batch 130 loss: 0.0054768595728091896\n",
      "batch 140 loss: 0.004766989656491205\n",
      "LOSS train 0.004766989656491205 valid 0.01676255278289318\n"
     ]
    }
   ],
   "source": [
    "#set training parameters\n",
    "model = CNN()\n",
    "criterion = torch.nn.NLLLoss()\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "num_epochs = 5\n",
    "\n",
    "best_val_loss = 10e6\n",
    "\n",
    "#train model\n",
    "for epoch in range(num_epochs):\n",
    "    print('EPOCH {}'.format(epoch+1))\n",
    "    \n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch()\n",
    "    \n",
    "    #check validation loss\n",
    "    model.train(False)\n",
    "    running_val_loss = 0.\n",
    "    for i, val_data in enumerate(val_dataloader):\n",
    "        val_inputs, val_target = val_data\n",
    "        val_outputs = model(val_inputs)[0]\n",
    "        \n",
    "        #relabeling target values so 5-->0 and 8-->1\n",
    "        new_val_target = torch.tensor([0 if label==5 else 1 for label in val_target])\n",
    "        \n",
    "        val_loss = criterion(val_outputs, new_val_target)\n",
    "        running_val_loss += val_loss\n",
    "    \n",
    "    avg_val_loss = running_val_loss / (i+1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_val_loss))\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f1fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
