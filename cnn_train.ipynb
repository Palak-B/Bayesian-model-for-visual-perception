{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c1e85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1bb887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MNIST training and test datasets\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data',\n",
    "                              train=True,\n",
    "                              download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "test_dataset = datasets.MNIST(root='data',\n",
    "                              train=False,\n",
    "                              download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809083b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training dataset into training and validation sets\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, \n",
    "                                                           [50000, 10000],\n",
    "                                                           generator=torch.Generator().manual_seed(11))\n",
    "\n",
    "#create filters for datasets so only 5s and 8s are included\n",
    "train_filter = [idx for idx, sample in enumerate(train_dataset) if sample[1] in [5,8]]\n",
    "val_filter = [idx for idx, sample in enumerate(val_dataset) if sample[1] in [5,8]]\n",
    "\n",
    "\n",
    "#create dataloaders using filtered training and test datasets\n",
    "train_dataloader = torch.utils.data.DataLoader(torch.utils.data.Subset(train_dataset, train_filter),\n",
    "                                              batch_size = 64)\n",
    "val_dataloader = torch.utils.data.DataLoader(torch.utils.data.Subset(val_dataset, val_filter),\n",
    "                                              batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7bbf691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training labels:  tensor([8, 8, 5, 5, 8, 8, 5, 5, 8, 5, 5, 8, 5, 5, 5, 5, 5, 5, 8, 5, 5, 8, 5, 8,\n",
      "        8, 5, 5, 8, 5, 8, 8, 8, 5, 5, 8, 8, 5, 5, 5, 5, 8, 8, 8, 8, 5, 8, 5, 8,\n",
      "        8, 8, 8, 8, 5, 5, 5, 5, 8, 8, 8, 8, 5, 8, 5, 5])\n",
      "val labels:  tensor([5, 5, 5, 5, 5, 8, 8, 8, 5, 5, 8, 8, 5, 5, 8, 5, 8, 8, 5, 8, 5, 8, 8, 5,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 5, 8, 5, 5, 5, 8, 8, 8, 8, 8, 5, 8, 5,\n",
      "        8, 8, 8, 5, 5, 8, 5, 5, 8, 5, 5, 8, 8, 5, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "#check to make sure dataloaders are properly filtered\n",
    "\n",
    "train_labels = next(iter(train_dataloader))[1]\n",
    "print('training labels: ', train_labels)\n",
    "\n",
    "val_labels = next(iter(val_dataloader))[1]\n",
    "print('val labels: ', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc2049e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define network for image processing\n",
    "#outputs are in the form of (log class probabilities, hidden features)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # 10 channels in first convolution layer\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) # 20 channels in second conv. layer\n",
    "        self.fc1 = nn.Linear(320, 10) # 10 hidden units in first fully-connected layer\n",
    "        self.fc2 = nn.Linear(10, 2) # 2 output units\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # first convolutional layer\n",
    "        h_conv1 = self.conv1(x)\n",
    "        h_conv1 = F.relu(h_conv1)\n",
    "        h_conv1_pool = F.max_pool2d(h_conv1, 2)\n",
    "\n",
    "        # second convolutional layer\n",
    "        h_conv2 = self.conv2(h_conv1_pool)\n",
    "        h_conv2 = F.relu(h_conv2)\n",
    "        h_conv2_pool = F.max_pool2d(h_conv2, 2)\n",
    "\n",
    "        # fully-connected layer\n",
    "        h_fc1 = h_conv2_pool.view(-1, 320)\n",
    "        h_fc1 = self.fc1(h_fc1)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "        \n",
    "        # classifier output\n",
    "        output = self.fc2(h_fc1)\n",
    "        output = F.log_softmax(output,dim=1)\n",
    "        return output, h_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60ea82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training loop\n",
    "\n",
    "def train_one_epoch():\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        data = batch[0]\n",
    "        target = batch[1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[0]\n",
    "        \n",
    "        #relabeling target values so 5-->0 and 8-->1\n",
    "        new_target = torch.tensor([0 if label==5 else 1 for label in target])\n",
    "        \n",
    "        loss = criterion(output, new_target)\n",
    "        #print('loss: ', loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i%10 == 9:\n",
    "            last_loss = running_loss/10\n",
    "            print('batch {} loss: {}'.format(i+1, last_loss))\n",
    "            running_loss = 0.\n",
    "        \n",
    "            \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f82af24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "batch 10 loss: 0.6140370398759842\n",
      "batch 20 loss: 0.3843510627746582\n",
      "batch 30 loss: 0.18872250467538834\n",
      "batch 40 loss: 0.1199959971010685\n",
      "batch 50 loss: 0.11840065531432628\n",
      "batch 60 loss: 0.05159692857414484\n",
      "batch 70 loss: 0.08677796442061662\n",
      "batch 80 loss: 0.06210407223552465\n",
      "batch 90 loss: 0.03817955674603581\n",
      "batch 100 loss: 0.04360020002350211\n",
      "batch 110 loss: 0.03200101349502802\n",
      "batch 120 loss: 0.03342755315825343\n",
      "batch 130 loss: 0.0369915752671659\n",
      "batch 140 loss: 0.038491040095686914\n",
      "LOSS train 0.038491040095686914 valid 0.03981615602970123\n",
      "EPOCH 2\n",
      "batch 10 loss: 0.03209939012303949\n",
      "batch 20 loss: 0.046838431153446436\n",
      "batch 30 loss: 0.027229642495512962\n",
      "batch 40 loss: 0.031206665188074113\n",
      "batch 50 loss: 0.04585299580357969\n",
      "batch 60 loss: 0.015964054223150014\n",
      "batch 70 loss: 0.04624083014205098\n",
      "batch 80 loss: 0.039967200998216865\n",
      "batch 90 loss: 0.01574304080568254\n",
      "batch 100 loss: 0.015619005309417844\n",
      "batch 110 loss: 0.02012655879370868\n",
      "batch 120 loss: 0.015366165060549974\n",
      "batch 130 loss: 0.01138540847459808\n",
      "batch 140 loss: 0.021919089613948017\n",
      "LOSS train 0.021919089613948017 valid 0.022596361115574837\n",
      "EPOCH 3\n",
      "batch 10 loss: 0.01213670348515734\n",
      "batch 20 loss: 0.029308217018842696\n",
      "batch 30 loss: 0.01658258675597608\n",
      "batch 40 loss: 0.020564960280898957\n",
      "batch 50 loss: 0.02999525601044297\n",
      "batch 60 loss: 0.009457253816071898\n",
      "batch 70 loss: 0.03335441973758861\n",
      "batch 80 loss: 0.025019542407244445\n",
      "batch 90 loss: 0.01005238110665232\n",
      "batch 100 loss: 0.009974721167236567\n",
      "batch 110 loss: 0.018527868785895407\n",
      "batch 120 loss: 0.010608340986073017\n",
      "batch 130 loss: 0.00586161034880206\n",
      "batch 140 loss: 0.015303832746576517\n",
      "LOSS train 0.015303832746576517 valid 0.01961258426308632\n",
      "EPOCH 4\n",
      "batch 10 loss: 0.007259759621229022\n",
      "batch 20 loss: 0.021246231533586978\n",
      "batch 30 loss: 0.012288204743526876\n",
      "batch 40 loss: 0.015267541888169945\n",
      "batch 50 loss: 0.018322593672201036\n",
      "batch 60 loss: 0.005931093555409462\n",
      "batch 70 loss: 0.027490263502113522\n",
      "batch 80 loss: 0.013296138064470142\n",
      "batch 90 loss: 0.00742767279734835\n",
      "batch 100 loss: 0.006191194243729115\n",
      "batch 110 loss: 0.01545414562569931\n",
      "batch 120 loss: 0.006859706877730787\n",
      "batch 130 loss: 0.0036721354583278297\n",
      "batch 140 loss: 0.00920401630573906\n",
      "LOSS train 0.00920401630573906 valid 0.017975596711039543\n",
      "EPOCH 5\n",
      "batch 10 loss: 0.004526420251931995\n",
      "batch 20 loss: 0.016116783791221678\n",
      "batch 30 loss: 0.010647834721021354\n",
      "batch 40 loss: 0.012538014206802473\n",
      "batch 50 loss: 0.013570089271524921\n",
      "batch 60 loss: 0.0035742976615438238\n",
      "batch 70 loss: 0.02327057399088517\n",
      "batch 80 loss: 0.008924066432518885\n",
      "batch 90 loss: 0.0062667567806784065\n",
      "batch 100 loss: 0.0043130538018886\n",
      "batch 110 loss: 0.01295821822131984\n",
      "batch 120 loss: 0.005296344636008143\n",
      "batch 130 loss: 0.0026331922243116425\n",
      "batch 140 loss: 0.005854698525217828\n",
      "LOSS train 0.005854698525217828 valid 0.018771953880786896\n"
     ]
    }
   ],
   "source": [
    "#set training parameters\n",
    "model = CNN()\n",
    "criterion = torch.nn.NLLLoss()\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "num_epochs = 5\n",
    "\n",
    "best_val_loss = 10e6\n",
    "\n",
    "#train model\n",
    "for epoch in range(num_epochs):\n",
    "    print('EPOCH {}'.format(epoch+1))\n",
    "    \n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch()\n",
    "    \n",
    "    #check validation loss\n",
    "    model.train(False)\n",
    "    running_val_loss = 0.\n",
    "    for i, val_data in enumerate(val_dataloader):\n",
    "        val_inputs, val_target = val_data\n",
    "        val_outputs = model(val_inputs)[0]\n",
    "        \n",
    "        #relabeling target values so 5-->0 and 8-->1\n",
    "        new_val_target = torch.tensor([0 if label==5 else 1 for label in val_target])\n",
    "        \n",
    "        val_loss = criterion(val_outputs, new_val_target)\n",
    "        running_val_loss += val_loss\n",
    "    \n",
    "    avg_val_loss = running_val_loss / (i+1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_val_loss))\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b21f1fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
      "0      8   9.703371        0.0   2.013049   0.000000        0.0        0.0   \n",
      "1      8  13.691728        0.0   0.000000   0.000000        0.0        0.0   \n",
      "2      5   0.000000        0.0  13.714252  12.262377        0.0        0.0   \n",
      "3      5   0.000000        0.0  11.346357   8.525431        0.0        0.0   \n",
      "4      8   5.615644        0.0   7.408042   3.080934        0.0        0.0   \n",
      "\n",
      "   feature_7  feature_8  feature_9  feature_10  \n",
      "0   3.053184  10.997141        0.0         0.0  \n",
      "1   0.305161  14.574522        0.0         0.0  \n",
      "2  16.002954   0.000000        0.0         0.0  \n",
      "3  12.898945   0.332094        0.0         0.0  \n",
      "4   6.832582   6.443026        0.0         0.0  \n"
     ]
    }
   ],
   "source": [
    "#extract hidden features from training dataset\n",
    "\n",
    "#initialize results array\n",
    "hidden_features_results = np.empty([len(train_dataloader)*64, 11])\n",
    "\n",
    "#iterate through training dataloader and collect hidden features\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    data = batch[0]\n",
    "    target = batch[1]\n",
    "    \n",
    "    log_probs, hidden_features = model(data)\n",
    "    \n",
    "    batch_results = np.empty([len(target), 11])\n",
    "    \n",
    "    batch_results[:,0] = target\n",
    "    batch_results[:,1:] = hidden_features.detach().numpy()\n",
    "    \n",
    "    hidden_features_results[i*64:(i*64)+len(target)] = batch_results\n",
    "    \n",
    "#store results in dataframe\n",
    "cols = ['label',\n",
    "        'feature_1',\n",
    "        'feature_2',\n",
    "        'feature_3',\n",
    "        'feature_4',\n",
    "        'feature_5',\n",
    "        'feature_6',\n",
    "        'feature_7',\n",
    "        'feature_8',\n",
    "        'feature_9',\n",
    "        'feature_10']\n",
    "\n",
    "hidden_features_df = pd.DataFrame(data = hidden_features_results, columns = cols)\n",
    "hidden_features_df['label'] = hidden_features_df['label'].astype(int)\n",
    "hidden_features_df = hidden_features_df[np.logical_or(hidden_features_df['label']==5, hidden_features_df['label']==8)]\n",
    "print(hidden_features_df.head())\n",
    "\n",
    "#save dataframe as csv\n",
    "hidden_features_df.to_csv('hidden_features_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c829e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
