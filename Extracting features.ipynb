{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "RQYXz3gP12QZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pic_one = str('/content/26.jpg')\n",
        "pic_two = str('/content/img_105.jpg')"
      ],
      "metadata": {
        "id": "vvCohDW43SJP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model\n",
        "model = models.resnet18(pretrained=True)\n",
        "# Use the model object to select the desired layer\n",
        "layer = model._modules.get('avgpool')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzfcCh3k3ZDY",
        "outputId": "4c23b8dd-ac36-4edd-f7d8-295d34148555"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 216MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bxiM5YS3ZXJ",
        "outputId": "7dce60d5-5374-407b-8b88-257769547c19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = transforms.Resize((224, 224))\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "to_tensor = transforms.ToTensor()"
      ],
      "metadata": {
        "id": "7Pj4adEf3cKm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector(image_name):\n",
        "    # 1. Load the image with Pillow library\n",
        "    img = Image.open(image_name)\n",
        "    # 2. Create a PyTorch Variable with the transformed image\n",
        "    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))\n",
        "    # 3. Create a vector of zeros that will hold our feature vector\n",
        "    #    The 'avgpool' layer has an output size of 512\n",
        "    my_embedding = torch.zeros(512)\n",
        "    # 4. Define a function that will copy the output of a layer\n",
        "    def copy_data(m, i, o):\n",
        "        my_embedding.copy_(o.data.reshape(o.data.size(1)))\n",
        "    # 5. Attach that function to our selected layer\n",
        "    h = layer.register_forward_hook(copy_data)\n",
        "    # 6. Run the model on our transformed image\n",
        "    model(t_img)\n",
        "    # 7. Detach our copy function from the layer\n",
        "    h.remove()\n",
        "    # 8. Return the feature vector\n",
        "    return my_embedding"
      ],
      "metadata": {
        "id": "fGuX7XtG3eMh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pic_one_vector = get_vector(pic_one)\n",
        "#pic_two_vector = get_vector(pic_two)"
      ],
      "metadata": {
        "id": "ucVJQZiL3igG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pic_one_vector\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTux0rP8JSOB",
        "outputId": "d114673b-b3dd-409f-efd3-b1b516092231"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0401e+00, 3.9349e-01, 7.6473e-01, 1.0625e-01, 1.4177e-01, 1.0908e-01,\n",
              "        1.0564e+00, 4.7352e+00, 2.6898e-02, 3.9582e+00, 2.2217e-01, 9.3210e-02,\n",
              "        0.0000e+00, 5.9293e-02, 5.0496e-02, 2.5217e+00, 6.6857e-01, 1.4738e+00,\n",
              "        8.8085e-03, 1.3152e+00, 1.0413e-03, 2.1327e-01, 3.6598e-01, 2.3528e+00,\n",
              "        6.4570e-03, 1.5999e+00, 8.9942e-02, 1.4803e+00, 2.2670e-01, 1.6051e+00,\n",
              "        1.3308e-01, 7.0444e-02, 5.5934e-02, 2.1423e-01, 1.5822e+00, 1.5061e+00,\n",
              "        1.8242e-01, 4.5989e-02, 5.0539e-02, 5.4030e+00, 5.0134e-01, 2.4174e-01,\n",
              "        1.6655e-01, 1.7960e-01, 7.1745e-02, 1.2742e+00, 3.9478e-02, 1.3803e+00,\n",
              "        7.5369e-01, 2.1739e-02, 0.0000e+00, 2.1307e+00, 2.6124e-01, 1.1327e+00,\n",
              "        4.9237e-01, 1.0753e+00, 3.4015e+00, 1.2134e-02, 1.4705e+00, 1.2073e+00,\n",
              "        7.2657e-01, 2.0881e-01, 4.2857e-01, 1.3095e+00, 5.9206e-02, 2.1506e+00,\n",
              "        8.2775e-02, 1.7095e-01, 4.3121e-01, 9.1391e-01, 3.6785e-02, 2.1096e+00,\n",
              "        2.7961e-01, 1.4101e-01, 5.7482e-01, 2.5009e+00, 2.5466e-01, 3.2192e-01,\n",
              "        3.4127e-01, 7.4845e-02, 1.8107e-01, 3.7097e-01, 1.1475e+00, 2.5414e+00,\n",
              "        1.0133e-01, 8.5468e-02, 2.0465e-01, 7.7749e-02, 6.5177e-01, 2.3935e+00,\n",
              "        6.9219e-01, 3.5729e-01, 6.9084e-01, 3.5478e-01, 1.4411e+00, 2.3286e-01,\n",
              "        6.3659e-01, 3.6756e+00, 1.0597e+00, 9.6754e-02, 2.0645e-02, 5.7306e-01,\n",
              "        3.6380e-02, 2.5182e-01, 1.8803e+00, 1.2174e+00, 5.8126e-01, 2.2510e-01,\n",
              "        1.5798e-01, 6.1589e-01, 7.7333e-01, 8.4974e-01, 1.7630e+00, 6.0970e-01,\n",
              "        3.1430e-02, 2.5319e-02, 2.4684e-01, 3.9379e-01, 0.0000e+00, 2.8558e-01,\n",
              "        3.8741e-01, 4.6356e-01, 3.9250e-02, 4.4492e-01, 6.5011e-01, 9.7493e-02,\n",
              "        8.1774e-02, 1.0715e+00, 9.0726e-01, 2.8222e-04, 1.3181e+00, 1.4800e-02,\n",
              "        1.3765e+00, 1.4625e+00, 1.5352e+00, 1.4166e+00, 1.7136e-01, 1.6877e-01,\n",
              "        1.9131e-01, 2.8945e-01, 2.1128e-02, 2.5401e-01, 2.4470e+00, 2.3799e-01,\n",
              "        3.5062e-01, 4.3007e-02, 9.2594e-01, 6.4441e-01, 7.4992e-03, 1.4860e+00,\n",
              "        6.0725e-01, 1.3864e-01, 4.4907e-01, 2.8519e-01, 1.2862e+00, 0.0000e+00,\n",
              "        7.8979e-04, 8.9016e-02, 3.8078e-02, 8.2720e-02, 2.0864e+00, 1.4594e+00,\n",
              "        0.0000e+00, 7.7303e-01, 8.9246e-02, 9.5274e-02, 6.3857e-03, 1.6216e+00,\n",
              "        3.1731e-01, 5.6506e-01, 9.5261e-01, 2.4015e-02, 6.8601e-02, 2.1188e-01,\n",
              "        4.1615e-01, 1.0382e+00, 1.0684e-02, 2.5425e-01, 5.1458e-01, 7.5958e-01,\n",
              "        1.5463e+00, 6.7671e-01, 3.1140e+00, 5.3997e-01, 3.5233e+00, 1.5878e-01,\n",
              "        8.9646e-02, 1.1259e+00, 5.0980e-01, 1.2851e-01, 1.2866e-01, 6.0782e-01,\n",
              "        5.0665e-01, 2.5583e+00, 6.3927e-01, 6.9466e-01, 1.7187e+00, 3.5575e-01,\n",
              "        0.0000e+00, 1.8311e+00, 2.5636e-01, 5.3205e-01, 3.7491e-01, 1.8661e+00,\n",
              "        2.5939e-01, 1.6209e+00, 1.3886e+00, 1.4727e-01, 8.3218e-02, 1.1273e+00,\n",
              "        3.0376e-02, 5.4252e-01, 1.4629e+00, 4.8092e-01, 0.0000e+00, 3.1529e-02,\n",
              "        1.6390e+00, 7.7997e-01, 5.9431e-02, 4.6570e-01, 2.8690e-01, 8.1943e-01,\n",
              "        9.3476e-02, 3.3694e-01, 1.8067e-01, 1.3823e-02, 1.2632e+00, 3.1700e-01,\n",
              "        1.1996e-02, 1.3140e+00, 1.1606e+00, 1.0980e-01, 6.6553e-02, 3.6014e-01,\n",
              "        4.0392e-02, 8.0626e-01, 9.4287e-02, 3.1625e-01, 1.1863e-01, 2.7547e-01,\n",
              "        2.6860e-03, 1.0766e+00, 1.8056e+00, 9.5225e-02, 2.3196e-02, 4.2837e-02,\n",
              "        2.6651e-01, 4.2080e-03, 7.2657e-01, 1.1119e+00, 1.7601e+00, 1.5699e-01,\n",
              "        1.3528e+00, 5.3354e-01, 5.9747e-01, 6.8888e-01, 9.3277e-01, 3.5981e-01,\n",
              "        8.5253e-01, 1.6233e-01, 4.3849e-02, 8.7766e-01, 1.7269e+00, 3.4636e-01,\n",
              "        1.0189e+00, 1.1589e+00, 6.3404e-01, 4.9846e-01, 9.3448e-03, 4.2583e-02,\n",
              "        1.0299e+00, 1.5623e-01, 1.0759e+00, 3.5645e+00, 1.5684e+00, 7.4930e-02,\n",
              "        6.0092e-01, 1.1347e-01, 2.4851e+00, 1.5330e+00, 1.7476e+00, 3.7894e-01,\n",
              "        2.8401e-01, 5.5873e-01, 1.3494e+00, 4.1626e-01, 1.6446e+00, 7.6964e-01,\n",
              "        3.5604e-01, 1.0356e+00, 7.0384e-01, 9.0726e-01, 2.0323e-01, 6.8386e-01,\n",
              "        1.2111e+00, 2.8017e-01, 6.2009e-01, 1.0668e-01, 3.2114e+00, 1.9625e+00,\n",
              "        0.0000e+00, 1.2051e+00, 1.8270e-01, 1.9878e-02, 1.6329e-01, 6.8670e-03,\n",
              "        3.7456e-01, 3.2802e-01, 1.2914e+00, 5.1483e-01, 1.3272e+00, 4.4179e-01,\n",
              "        1.1694e+00, 7.5413e-01, 3.6196e-01, 2.1656e-01, 8.2862e-03, 1.2558e+00,\n",
              "        7.2823e-02, 2.7827e+00, 1.5186e+00, 3.8835e-02, 1.1097e+00, 9.3509e-03,\n",
              "        8.6985e-01, 6.9125e-01, 5.0487e-01, 2.9080e-03, 1.5397e+00, 3.4368e-01,\n",
              "        4.4844e-02, 9.6029e-01, 2.0570e-01, 1.1577e+00, 4.9747e+00, 2.8656e-01,\n",
              "        1.0710e+00, 1.5586e-01, 1.0528e-01, 8.9386e-03, 2.3911e+00, 1.6183e-01,\n",
              "        1.3083e-02, 2.0538e+00, 9.1947e-01, 1.0227e+00, 6.1862e-01, 1.2097e+00,\n",
              "        2.7389e-02, 8.2782e-01, 1.6030e-02, 1.7450e-01, 5.4850e-01, 9.6660e-01,\n",
              "        1.4127e-01, 1.2519e+00, 1.0353e-01, 5.1232e-03, 3.8693e-01, 1.3998e+00,\n",
              "        1.5425e+00, 3.0366e-01, 3.9104e-01, 1.5202e+00, 4.2849e-02, 4.2827e-03,\n",
              "        1.5332e+00, 1.3744e-01, 4.7994e-02, 3.9089e-01, 1.4287e+00, 5.9145e-01,\n",
              "        2.9246e-01, 1.0244e+00, 3.1371e-01, 8.0983e-03, 6.3214e-01, 1.2129e-01,\n",
              "        7.4745e-01, 1.0352e+00, 3.7915e-01, 9.6545e-01, 1.6737e-02, 3.7353e-02,\n",
              "        1.5186e+00, 7.0982e-01, 1.0789e-03, 1.6008e+00, 8.4382e-02, 5.8681e-01,\n",
              "        1.0130e+00, 1.8515e+00, 7.6966e-01, 7.7954e-01, 4.8593e-01, 7.4051e-01,\n",
              "        2.8217e-01, 1.8609e-01, 1.0945e-02, 5.2433e-01, 5.3153e-01, 1.3997e+00,\n",
              "        2.4378e-01, 1.5058e-01, 1.1208e-01, 7.1943e-01, 7.8880e-01, 9.2147e-01,\n",
              "        1.2578e-02, 2.3400e-01, 2.2713e-02, 2.3016e-01, 7.5986e-01, 1.7601e-01,\n",
              "        6.7271e-01, 1.8129e+00, 6.8623e-01, 1.3738e-01, 5.3689e-01, 8.4836e-03,\n",
              "        1.1140e-01, 1.1765e-01, 2.4924e-01, 3.3753e-01, 8.3678e-01, 7.4747e-01,\n",
              "        1.8888e+00, 1.6759e-02, 1.0581e+00, 2.3131e-01, 2.4318e-01, 2.1012e-01,\n",
              "        2.0974e-01, 1.6682e-01, 3.0754e-01, 4.9031e+00, 1.4107e+00, 4.2940e-01,\n",
              "        3.2844e-01, 5.8280e-02, 3.0539e-01, 2.1845e-02, 6.4363e-02, 9.3343e-01,\n",
              "        2.8806e-02, 6.9272e-01, 8.4424e-02, 1.8463e+00, 1.2294e+00, 1.9242e+00,\n",
              "        4.5578e-01, 1.3017e-02, 9.4369e-01, 8.9172e-01, 4.8927e-01, 2.4223e+00,\n",
              "        1.0801e-01, 6.6013e-01, 6.1414e-02, 1.2539e+00, 1.8057e-02, 0.0000e+00,\n",
              "        0.0000e+00, 1.7877e-02, 3.4455e-01, 1.3614e+00, 1.0577e+00, 6.1774e-01,\n",
              "        5.4313e-01, 4.1990e-01, 1.6285e+00, 9.3428e-01, 6.5414e-01, 1.4129e-01,\n",
              "        1.1924e+00, 8.5388e-02, 9.7297e-02, 3.2530e-01, 2.7836e-01, 1.1324e-01,\n",
              "        8.3020e+00, 1.6950e+00, 2.8964e-01, 8.7953e-01, 3.1434e-01, 8.5030e-01,\n",
              "        1.7775e-01, 1.3735e+00, 1.1402e+00, 0.0000e+00, 5.0993e-01, 2.4893e+00,\n",
              "        5.4067e-01, 1.1384e+00, 4.8738e-01, 7.2166e-02, 1.0860e-01, 2.0629e-01,\n",
              "        1.4989e+00, 1.7270e-02, 5.0486e-02, 2.3914e-01, 1.8884e-01, 4.5176e-01,\n",
              "        5.7189e-01, 9.5614e-02, 4.3371e-01, 1.1064e+00, 5.7796e-01, 5.7464e-01,\n",
              "        3.5420e+00, 2.7795e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using PyTorch Cosine Similarity\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "cos_sim = cos(pic_one_vector.unsqueeze(0),\n",
        "              pic_two_vector.unsqueeze(0))\n",
        "print('\\nCosine similarity: {0}\\n'.format(cos_sim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmTz0iHV3i4A",
        "outputId": "e77d6ab7-a776-45a9-ceb4-0e9be5b78adb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cosine similarity: tensor([1.])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pic_one_vector"
      ],
      "metadata": {
        "id": "LLiVq5rh3k6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26edf29b-9654-4b55-f19e-ef83a30efa0e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0401e+00, 3.9349e-01, 7.6473e-01, 1.0625e-01, 1.4177e-01, 1.0908e-01,\n",
              "        1.0564e+00, 4.7352e+00, 2.6898e-02, 3.9582e+00, 2.2217e-01, 9.3210e-02,\n",
              "        0.0000e+00, 5.9293e-02, 5.0496e-02, 2.5217e+00, 6.6857e-01, 1.4738e+00,\n",
              "        8.8085e-03, 1.3152e+00, 1.0413e-03, 2.1327e-01, 3.6598e-01, 2.3528e+00,\n",
              "        6.4570e-03, 1.5999e+00, 8.9942e-02, 1.4803e+00, 2.2670e-01, 1.6051e+00,\n",
              "        1.3308e-01, 7.0444e-02, 5.5934e-02, 2.1423e-01, 1.5822e+00, 1.5061e+00,\n",
              "        1.8242e-01, 4.5989e-02, 5.0539e-02, 5.4030e+00, 5.0134e-01, 2.4174e-01,\n",
              "        1.6655e-01, 1.7960e-01, 7.1745e-02, 1.2742e+00, 3.9478e-02, 1.3803e+00,\n",
              "        7.5369e-01, 2.1739e-02, 0.0000e+00, 2.1307e+00, 2.6124e-01, 1.1327e+00,\n",
              "        4.9237e-01, 1.0753e+00, 3.4015e+00, 1.2134e-02, 1.4705e+00, 1.2073e+00,\n",
              "        7.2657e-01, 2.0881e-01, 4.2857e-01, 1.3095e+00, 5.9206e-02, 2.1506e+00,\n",
              "        8.2775e-02, 1.7095e-01, 4.3121e-01, 9.1391e-01, 3.6785e-02, 2.1096e+00,\n",
              "        2.7961e-01, 1.4101e-01, 5.7482e-01, 2.5009e+00, 2.5466e-01, 3.2192e-01,\n",
              "        3.4127e-01, 7.4845e-02, 1.8107e-01, 3.7097e-01, 1.1475e+00, 2.5414e+00,\n",
              "        1.0133e-01, 8.5468e-02, 2.0465e-01, 7.7749e-02, 6.5177e-01, 2.3935e+00,\n",
              "        6.9219e-01, 3.5729e-01, 6.9084e-01, 3.5478e-01, 1.4411e+00, 2.3286e-01,\n",
              "        6.3659e-01, 3.6756e+00, 1.0597e+00, 9.6754e-02, 2.0645e-02, 5.7306e-01,\n",
              "        3.6380e-02, 2.5182e-01, 1.8803e+00, 1.2174e+00, 5.8126e-01, 2.2510e-01,\n",
              "        1.5798e-01, 6.1589e-01, 7.7333e-01, 8.4974e-01, 1.7630e+00, 6.0970e-01,\n",
              "        3.1430e-02, 2.5319e-02, 2.4684e-01, 3.9379e-01, 0.0000e+00, 2.8558e-01,\n",
              "        3.8741e-01, 4.6356e-01, 3.9250e-02, 4.4492e-01, 6.5011e-01, 9.7493e-02,\n",
              "        8.1774e-02, 1.0715e+00, 9.0726e-01, 2.8222e-04, 1.3181e+00, 1.4800e-02,\n",
              "        1.3765e+00, 1.4625e+00, 1.5352e+00, 1.4166e+00, 1.7136e-01, 1.6877e-01,\n",
              "        1.9131e-01, 2.8945e-01, 2.1128e-02, 2.5401e-01, 2.4470e+00, 2.3799e-01,\n",
              "        3.5062e-01, 4.3007e-02, 9.2594e-01, 6.4441e-01, 7.4992e-03, 1.4860e+00,\n",
              "        6.0725e-01, 1.3864e-01, 4.4907e-01, 2.8519e-01, 1.2862e+00, 0.0000e+00,\n",
              "        7.8979e-04, 8.9016e-02, 3.8078e-02, 8.2720e-02, 2.0864e+00, 1.4594e+00,\n",
              "        0.0000e+00, 7.7303e-01, 8.9246e-02, 9.5274e-02, 6.3857e-03, 1.6216e+00,\n",
              "        3.1731e-01, 5.6506e-01, 9.5261e-01, 2.4015e-02, 6.8601e-02, 2.1188e-01,\n",
              "        4.1615e-01, 1.0382e+00, 1.0684e-02, 2.5425e-01, 5.1458e-01, 7.5958e-01,\n",
              "        1.5463e+00, 6.7671e-01, 3.1140e+00, 5.3997e-01, 3.5233e+00, 1.5878e-01,\n",
              "        8.9646e-02, 1.1259e+00, 5.0980e-01, 1.2851e-01, 1.2866e-01, 6.0782e-01,\n",
              "        5.0665e-01, 2.5583e+00, 6.3927e-01, 6.9466e-01, 1.7187e+00, 3.5575e-01,\n",
              "        0.0000e+00, 1.8311e+00, 2.5636e-01, 5.3205e-01, 3.7491e-01, 1.8661e+00,\n",
              "        2.5939e-01, 1.6209e+00, 1.3886e+00, 1.4727e-01, 8.3218e-02, 1.1273e+00,\n",
              "        3.0376e-02, 5.4252e-01, 1.4629e+00, 4.8092e-01, 0.0000e+00, 3.1529e-02,\n",
              "        1.6390e+00, 7.7997e-01, 5.9431e-02, 4.6570e-01, 2.8690e-01, 8.1943e-01,\n",
              "        9.3476e-02, 3.3694e-01, 1.8067e-01, 1.3823e-02, 1.2632e+00, 3.1700e-01,\n",
              "        1.1996e-02, 1.3140e+00, 1.1606e+00, 1.0980e-01, 6.6553e-02, 3.6014e-01,\n",
              "        4.0392e-02, 8.0626e-01, 9.4287e-02, 3.1625e-01, 1.1863e-01, 2.7547e-01,\n",
              "        2.6860e-03, 1.0766e+00, 1.8056e+00, 9.5225e-02, 2.3196e-02, 4.2837e-02,\n",
              "        2.6651e-01, 4.2080e-03, 7.2657e-01, 1.1119e+00, 1.7601e+00, 1.5699e-01,\n",
              "        1.3528e+00, 5.3354e-01, 5.9747e-01, 6.8888e-01, 9.3277e-01, 3.5981e-01,\n",
              "        8.5253e-01, 1.6233e-01, 4.3849e-02, 8.7766e-01, 1.7269e+00, 3.4636e-01,\n",
              "        1.0189e+00, 1.1589e+00, 6.3404e-01, 4.9846e-01, 9.3448e-03, 4.2583e-02,\n",
              "        1.0299e+00, 1.5623e-01, 1.0759e+00, 3.5645e+00, 1.5684e+00, 7.4930e-02,\n",
              "        6.0092e-01, 1.1347e-01, 2.4851e+00, 1.5330e+00, 1.7476e+00, 3.7894e-01,\n",
              "        2.8401e-01, 5.5873e-01, 1.3494e+00, 4.1626e-01, 1.6446e+00, 7.6964e-01,\n",
              "        3.5604e-01, 1.0356e+00, 7.0384e-01, 9.0726e-01, 2.0323e-01, 6.8386e-01,\n",
              "        1.2111e+00, 2.8017e-01, 6.2009e-01, 1.0668e-01, 3.2114e+00, 1.9625e+00,\n",
              "        0.0000e+00, 1.2051e+00, 1.8270e-01, 1.9878e-02, 1.6329e-01, 6.8670e-03,\n",
              "        3.7456e-01, 3.2802e-01, 1.2914e+00, 5.1483e-01, 1.3272e+00, 4.4179e-01,\n",
              "        1.1694e+00, 7.5413e-01, 3.6196e-01, 2.1656e-01, 8.2862e-03, 1.2558e+00,\n",
              "        7.2823e-02, 2.7827e+00, 1.5186e+00, 3.8835e-02, 1.1097e+00, 9.3509e-03,\n",
              "        8.6985e-01, 6.9125e-01, 5.0487e-01, 2.9080e-03, 1.5397e+00, 3.4368e-01,\n",
              "        4.4844e-02, 9.6029e-01, 2.0570e-01, 1.1577e+00, 4.9747e+00, 2.8656e-01,\n",
              "        1.0710e+00, 1.5586e-01, 1.0528e-01, 8.9386e-03, 2.3911e+00, 1.6183e-01,\n",
              "        1.3083e-02, 2.0538e+00, 9.1947e-01, 1.0227e+00, 6.1862e-01, 1.2097e+00,\n",
              "        2.7389e-02, 8.2782e-01, 1.6030e-02, 1.7450e-01, 5.4850e-01, 9.6660e-01,\n",
              "        1.4127e-01, 1.2519e+00, 1.0353e-01, 5.1232e-03, 3.8693e-01, 1.3998e+00,\n",
              "        1.5425e+00, 3.0366e-01, 3.9104e-01, 1.5202e+00, 4.2849e-02, 4.2827e-03,\n",
              "        1.5332e+00, 1.3744e-01, 4.7994e-02, 3.9089e-01, 1.4287e+00, 5.9145e-01,\n",
              "        2.9246e-01, 1.0244e+00, 3.1371e-01, 8.0983e-03, 6.3214e-01, 1.2129e-01,\n",
              "        7.4745e-01, 1.0352e+00, 3.7915e-01, 9.6545e-01, 1.6737e-02, 3.7353e-02,\n",
              "        1.5186e+00, 7.0982e-01, 1.0789e-03, 1.6008e+00, 8.4382e-02, 5.8681e-01,\n",
              "        1.0130e+00, 1.8515e+00, 7.6966e-01, 7.7954e-01, 4.8593e-01, 7.4051e-01,\n",
              "        2.8217e-01, 1.8609e-01, 1.0945e-02, 5.2433e-01, 5.3153e-01, 1.3997e+00,\n",
              "        2.4378e-01, 1.5058e-01, 1.1208e-01, 7.1943e-01, 7.8880e-01, 9.2147e-01,\n",
              "        1.2578e-02, 2.3400e-01, 2.2713e-02, 2.3016e-01, 7.5986e-01, 1.7601e-01,\n",
              "        6.7271e-01, 1.8129e+00, 6.8623e-01, 1.3738e-01, 5.3689e-01, 8.4836e-03,\n",
              "        1.1140e-01, 1.1765e-01, 2.4924e-01, 3.3753e-01, 8.3678e-01, 7.4747e-01,\n",
              "        1.8888e+00, 1.6759e-02, 1.0581e+00, 2.3131e-01, 2.4318e-01, 2.1012e-01,\n",
              "        2.0974e-01, 1.6682e-01, 3.0754e-01, 4.9031e+00, 1.4107e+00, 4.2940e-01,\n",
              "        3.2844e-01, 5.8280e-02, 3.0539e-01, 2.1845e-02, 6.4363e-02, 9.3343e-01,\n",
              "        2.8806e-02, 6.9272e-01, 8.4424e-02, 1.8463e+00, 1.2294e+00, 1.9242e+00,\n",
              "        4.5578e-01, 1.3017e-02, 9.4369e-01, 8.9172e-01, 4.8927e-01, 2.4223e+00,\n",
              "        1.0801e-01, 6.6013e-01, 6.1414e-02, 1.2539e+00, 1.8057e-02, 0.0000e+00,\n",
              "        0.0000e+00, 1.7877e-02, 3.4455e-01, 1.3614e+00, 1.0577e+00, 6.1774e-01,\n",
              "        5.4313e-01, 4.1990e-01, 1.6285e+00, 9.3428e-01, 6.5414e-01, 1.4129e-01,\n",
              "        1.1924e+00, 8.5388e-02, 9.7297e-02, 3.2530e-01, 2.7836e-01, 1.1324e-01,\n",
              "        8.3020e+00, 1.6950e+00, 2.8964e-01, 8.7953e-01, 3.1434e-01, 8.5030e-01,\n",
              "        1.7775e-01, 1.3735e+00, 1.1402e+00, 0.0000e+00, 5.0993e-01, 2.4893e+00,\n",
              "        5.4067e-01, 1.1384e+00, 4.8738e-01, 7.2166e-02, 1.0860e-01, 2.0629e-01,\n",
              "        1.4989e+00, 1.7270e-02, 5.0486e-02, 2.3914e-01, 1.8884e-01, 4.5176e-01,\n",
              "        5.7189e-01, 9.5614e-02, 4.3371e-01, 1.1064e+00, 5.7796e-01, 5.7464e-01,\n",
              "        3.5420e+00, 2.7795e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dJAsHhCdHoJc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}